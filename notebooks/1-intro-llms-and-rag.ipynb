{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0614425c-7f0f-4ecb-b8dc-f4c596488563",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Building Solutions with LLMs and Retrival Augmented Geenration (RAG): Introduction\n",
    "\n",
    "Welcome to this AI workshop! In this workshop we will get deeper into AI, RAG, chatbots, embeddings, evaluations and more.\n",
    "But first lets get used to this notebook enviroment.\n",
    "\n",
    "### Getting started with Jupyter Notebooks\n",
    "Notebooks are one of the most imporant tools in teh toolbelt of a data scientist of ML engineer.\n",
    "They are great for interacting with code and visualizing results.\n",
    "This file you are interacting with is a jupyter notebook. In Jupter you can have cells of either text or code.\n",
    "Cells with code can be run by pressing ctrl+enter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "1. Run the cell bellow a couple of times to get used to the notebook behaviour.\n",
    "2. Restart the kernel (Restart) button above and run again this code to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a if it does not exist\n",
    "if  'a' not in globals():\n",
    "    a = 42\n",
    "\n",
    "a = a+1\n",
    "\n",
    "# press ctrl+enter to run the cell, do that multiple times\n",
    "# notice how the variable `a` is persistent in the cell\n",
    "# you can run full programs in a notebook.\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LLMs\n",
    "Large Language Models (LLMs) are a type of machine learning models designed to understand and generate human language. They are trained on massive datasets of text to predict and generate language based on given prompts, learning patterns, structures, and relationships in text to produce human-like responses. They can be used to generate text, answer questions, and more.\n",
    "\n",
    "### MistralAI\n",
    "\n",
    "For this workshop we wil use [Mistral](https://mistral.ai/) langauge models, which are similar in concept to OpenAI's ChatGPT and Anthropic's Claude. If your installation finalized correctly you should have mistral already installed. \n",
    "The command below checks if mistral is installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: mistralai\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show mistralai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If for some reason mistral did not installed successfully you can get it (and all other dependencies of this workshop by installing it directly with the command below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/explodinggradients/ragas (from -r ../requirements.txt (line 11))\n",
      "  Cloning https://github.com/explodinggradients/ragas to /tmp/pip-req-build-iilplikf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/explodinggradients/ragas /tmp/pip-req-build-iilplikf\n",
      "  Resolved https://github.com/explodinggradients/ragas to commit 7d051437a1a5d8e9ad5c42252bf1debf51679140\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sentence-transformers~=3.2.0 (from -r ../requirements.txt (line 1))\n",
      "  Using cached sentence_transformers-3.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas~=2.2.3 (from -r ../requirements.txt (line 2))\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting mistralai~=1.1.0 (from -r ../requirements.txt (line 3))\n",
      "  Using cached mistralai-1.1.0-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting watchdog~=5.0.3 (from -r ../requirements.txt (line 4))\n",
      "  Using cached watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting PyPDF2~=3.0.1 (from -r ../requirements.txt (line 5))\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting streamlit~=1.39.0 (from -r ../requirements.txt (line 6))\n",
      "  Using cached streamlit-1.39.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting python-dotenv (from -r ../requirements.txt (line 7))\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: ipykernel in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from -r ../requirements.txt (line 8)) (6.29.5)\n",
      "Collecting fire (from -r ../requirements.txt (line 9))\n",
      "  Using cached fire-0.7.0-py3-none-any.whl\n",
      "Collecting langchain_mistralai (from -r ../requirements.txt (line 10))\n",
      "  Using cached langchain_mistralai-0.2.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting langchain-huggingface (from -r ../requirements.txt (line 12))\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting nltk (from -r ../requirements.txt (line 13))\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting rapidfuzz (from -r ../requirements.txt (line 14))\n",
      "  Using cached rapidfuzz-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting matplotlib (from -r ../requirements.txt (line 15))\n",
      "  Using cached matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting einops (from -r ../requirements.txt (line 16))\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting tqdm (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting scipy (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pillow-11.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from pandas~=2.2.3->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting eval-type-backport<0.3.0,>=0.2.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpx<0.28.0,>=0.27.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic<3.0.0,>=2.9.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached pydantic-2.10.1-py3-none-any.whl.metadata (169 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting typing-inspect<0.10.0,>=0.9.0 (from mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas~=2.2.3->-r ../requirements.txt (line 2)) (1.16.0)\n",
      "Collecting altair<6,>=4.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.0.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (24.2)\n",
      "Collecting Pillow (from sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting requests<3,>=2.27 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-extensions<5,>=4.3.0 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from streamlit~=1.39.0->-r ../requirements.txt (line 6)) (6.4.2)\n",
      "Requirement already satisfied: comm>=0.1.1 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (1.8.9)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (1.6.0)\n",
      "Requirement already satisfied: psutil in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (26.2.0)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 8)) (5.14.3)\n",
      "Collecting termcolor (from fire->-r ../requirements.txt (line 9))\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting httpx-sse<1,>=0.3.1 (from langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached langchain_core-0.3.21-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tokenizers<1,>=0.15.1 (from langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting datasets (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting tiktoken (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting langchain (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached langchain-0.3.8-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached langchain_community-0.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain_openai (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting appdirs (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting openai>1 (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached openai-1.55.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting joblib (from nltk->-r ../requirements.txt (line 13))\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->-r ../requirements.txt (line 13))\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib->-r ../requirements.txt (line 15))\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib->-r ../requirements.txt (line 15))\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib->-r ../requirements.txt (line 15))\n",
      "  Using cached fonttools-4.55.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib->-r ../requirements.txt (line 15))\n",
      "  Using cached kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib->-r ../requirements.txt (line 15))\n",
      "  Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached narwhals-1.14.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyio (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio (from httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: decorator in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.19.2)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r ../requirements.txt (line 8)) (4.3.6)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.125 (from langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached langsmith-0.1.145-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>1->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>1->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.1 (from pydantic<3.0.0,>=2.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting setuptools (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<0.10.0,>=0.9.0->mistralai~=1.1.0->-r ../requirements.txt (line 3))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached aiohttp-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached SQLAlchemy-2.0.36-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas~=2.2.3->-r ../requirements.txt (line 2))\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers~=3.2.0->-r ../requirements.txt (line 1))\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached yarl-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.8.4)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached rpds_py-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.15->langchain_mistralai->-r ../requirements.txt (line 10))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit~=1.39.0->-r ../requirements.txt (line 6))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.2.13)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain->ragas==0.2.7.dev1+g7d05143->-r ../requirements.txt (line 11))\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /workspaces/ai-quiz/.virtualenvironment/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 8)) (0.2.3)\n",
      "Using cached sentence_transformers-3.2.1-py3-none-any.whl (255 kB)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached mistralai-1.1.0-py3-none-any.whl (229 kB)\n",
      "Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Using cached watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Using cached streamlit-1.39.0-py2.py3-none-any.whl (8.7 MB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached langchain_mistralai-0.2.2-py3-none-any.whl (14 kB)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached rapidfuzz-3.10.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached matplotlib-3.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached fonttools-4.55.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached kiwisolver-1.4.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Using cached langchain_core-0.3.21-py3-none-any.whl (409 kB)\n",
      "Using cached openai-1.55.0-py3-none-any.whl (389 kB)\n",
      "Using cached pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached protobuf-5.28.3-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Using cached pyarrow-18.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "Using cached pydantic-2.10.1-py3-none-any.whl (455 kB)\n",
      "Using cached pydantic_core-2.27.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.20.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached torch-2.5.1-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "Using cached tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached langchain-0.3.8-py3-none-any.whl (1.0 MB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached langchain_community-0.3.8-py3-none-any.whl (2.4 MB)\n",
      "Using cached langchain_openai-0.2.9-py3-none-any.whl (50 kB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "Using cached scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached aiohttp-3.11.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached jiter-0.7.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Using cached langsmith-0.1.145-py3-none-any.whl (310 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached narwhals-1.14.2-py3-none-any.whl (225 kB)\n",
      "Using cached pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached SQLAlchemy-2.0.35-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (613 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached orjson-3.10.12-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (248 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached rpds_py-0.21.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (364 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Using cached yarl-1.18.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
      "Building wheels for collected packages: ragas\n",
      "  Building wheel for ragas (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ragas: filename=ragas-0.2.7.dev1+g7d05143-py3-none-any.whl size=155814 sha256=82c881c13063c522a01576c7882b33c9094eac9bd41e7c519d7336638405cace\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-solfacih/wheels/aa/db/38/a6cb74f407e90caf42004080da76661f852984de5b2ea8f5dd\n",
      "Successfully built ragas\n",
      "Installing collected packages: pytz, mpmath, appdirs, xxhash, watchdog, urllib3, tzdata, typing-extensions, tqdm, toml, threadpoolctl, termcolor, tenacity, sympy, sniffio, smmap, setuptools, safetensors, rpds-py, regex, rapidfuzz, pyyaml, python-dotenv, python-dateutil, pysbd, PyPDF2, pyparsing, pyarrow, protobuf, propcache, Pillow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, narwhals, mypy-extensions, multidict, mdurl, marshmallow, MarkupSafe, kiwisolver, jsonpointer, jsonpath-python, joblib, jiter, idna, httpx-sse, h11, greenlet, fsspec, frozenlist, fonttools, filelock, eval-type-backport, einops, distro, dill, cycler, click, charset-normalizer, certifi, cachetools, blinker, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, triton, SQLAlchemy, scipy, requests, referencing, pydantic-core, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nltk, multiprocess, markdown-it-py, jsonpatch, jinja2, httpcore, gitdb, fire, contourpy, anyio, aiosignal, tiktoken, scikit-learn, rich, requests-toolbelt, pydeck, pydantic, nvidia-cusolver-cu12, matplotlib, jsonschema-specifications, huggingface-hub, httpx, gitpython, dataclasses-json, aiohttp, torch, tokenizers, pydantic-settings, openai, mistralai, langsmith, jsonschema, transformers, langchain-core, datasets, altair, streamlit, sentence-transformers, langchain-text-splitters, langchain_openai, langchain_mistralai, langchain-huggingface, langchain, langchain-community, ragas\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "Successfully installed MarkupSafe-3.0.2 Pillow-10.4.0 PyPDF2-3.0.1 SQLAlchemy-2.0.35 aiohappyeyeballs-2.4.3 aiohttp-3.11.7 aiosignal-1.3.1 altair-5.5.0 annotated-types-0.7.0 anyio-4.6.2.post1 appdirs-1.4.4 attrs-24.2.0 blinker-1.9.0 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 contourpy-1.3.1 cycler-0.12.1 dataclasses-json-0.6.7 datasets-3.1.0 dill-0.3.8 distro-1.9.0 einops-0.8.0 eval-type-backport-0.2.0 filelock-3.16.1 fire-0.7.0 fonttools-4.55.0 frozenlist-1.5.0 fsspec-2024.9.0 gitdb-4.0.11 gitpython-3.1.43 greenlet-3.1.1 h11-0.14.0 httpcore-1.0.7 httpx-0.27.2 httpx-sse-0.4.0 huggingface-hub-0.26.2 idna-3.10 jinja2-3.1.4 jiter-0.7.1 joblib-1.4.2 jsonpatch-1.33 jsonpath-python-1.0.6 jsonpointer-3.0.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 kiwisolver-1.4.7 langchain-0.3.8 langchain-community-0.3.8 langchain-core-0.3.21 langchain-huggingface-0.1.2 langchain-text-splitters-0.3.2 langchain_mistralai-0.2.2 langchain_openai-0.2.9 langsmith-0.1.145 markdown-it-py-3.0.0 marshmallow-3.23.1 matplotlib-3.9.2 mdurl-0.1.2 mistralai-1.1.0 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 narwhals-1.14.2 networkx-3.4.2 nltk-3.9.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 openai-1.55.0 orjson-3.10.12 pandas-2.2.3 propcache-0.2.0 protobuf-5.28.3 pyarrow-18.0.0 pydantic-2.10.1 pydantic-core-2.27.1 pydantic-settings-2.6.1 pydeck-0.9.1 pyparsing-3.2.0 pysbd-0.3.4 python-dateutil-2.8.2 python-dotenv-1.0.1 pytz-2024.2 pyyaml-6.0.2 ragas-0.2.7.dev1+g7d05143 rapidfuzz-3.10.1 referencing-0.35.1 regex-2024.11.6 requests-2.32.3 requests-toolbelt-1.0.0 rich-13.9.4 rpds-py-0.21.0 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentence-transformers-3.2.1 setuptools-75.6.0 smmap-5.0.1 sniffio-1.3.1 streamlit-1.39.0 sympy-1.13.1 tenacity-9.0.0 termcolor-2.5.0 threadpoolctl-3.5.0 tiktoken-0.8.0 tokenizers-0.20.3 toml-0.10.2 torch-2.5.1 tqdm-4.67.0 transformers-4.46.3 triton-3.1.0 typing-extensions-4.12.2 typing-inspect-0.9.0 tzdata-2024.2 urllib3-2.2.3 watchdog-5.0.3 xxhash-3.5.0 yarl-1.18.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# run this command only if mistral is not installed \n",
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting access to Mistral LLMs\n",
    "\n",
    "LLMs are usually big and use a lot of computer resources the most common way to use them is to run them in teh cloud via network calls. For the sake of this workshop we will use the cloud version as well so we dont need to download big models.\n",
    "If you are interested in running llms locally check [ollama](https://github.com/ollama/ollama), arguably the most advanced project that does it.\n",
    "\n",
    "\n",
    "#### API key\n",
    "The second  step is to get a Mistral api key. You can find some APIs keys we prepared for this workshop in this [sheet](https://docs.google.com/spreadsheets/d/1ZwTpkG6OOuVrOx8nzPmgai_7Hwpo8Kun7yZrOmg_5K4/edit?gid=0#gid=0). Get the key (please write your name next to it in the sheet such that people know it is taken) and write it to the .env file using the command\n",
    "\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Use the cell below to write your api key into the file so you can use mistral.\n",
    "We write the variable into a file named .env. .env files a are a standard in python to load information like api keys and passwords that should not stay with the code for security reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write file .env\n",
    "\n",
    "mistral_api_key = 'REPLACE WITH YORU KEY'\n",
    "with open('../.env', 'w') as f:\n",
    "    f.write(f'MISTRAL_API_KEY=\"{mistral_api_key}\"')\n",
    "\n",
    "# make sure to delete teh API key from this cell before commiting the file so you dont save your key in the repo which is a security risk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if it worked\n",
    "\n",
    "Run the command below to check if writing the key worked.  It will trigger an error if it did not.  You might need to restart the kernel if it does not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /Users/jean.machado@getyourguide.com/prj/rag-workshop/.env\n",
      "The key is:  SDjaAJPOSn83H7Pr4mYwCSFZg2lw2xIS\n"
     ]
    }
   ],
   "source": [
    "# we now reload the the configuration file and it should show your key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env_file = find_dotenv()\n",
    "print(f\"Loading environment variables from {env_file}\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import os\n",
    "env = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# you should see your key here not\n",
    "print(\"The key is: \", env)\n",
    "\n",
    "if not env:\n",
    "    raise ValueError(\"The API key is not set. Please set the MISTRAL_API_KEY environment variable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running mistral\n",
    "Let's run the code below to import Mistral and initialize the Mistral client: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "\n",
    "# Retrieve the Mistral API key from the environment variables\n",
    "mistral_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "\n",
    "# Initialize the Mistral client with the API key\n",
    "mistral_client = Mistral(api_key=mistral_api_key)\n",
    "\n",
    "# The model below is the specific model we want to use\n",
    "model_name = \"mistral-small-latest\"\n",
    "\n",
    "# The code below defines a function `call_mistral_model` that sends a message to a Mistral model and returns the model's response text.\n",
    "response = mistral_client.chat.complete(\n",
    "    model = model_name,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"hello! What is your name?\",\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "# Extract only the text from the response\n",
    "response_text = response.choices[0].message.content\n",
    "print(response_text)\n",
    "\n",
    "## not how the result resembles natural language. Its the exact same concept as chatgpt.\n",
    "## feel free to play around with the prompt and see how the results change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having all of this complex code to call the llm, going forward, we can move this complexity to a python script.\n",
    "This is what we are doing from the code below on. We are replacing our calls to mistral api to calls to LargeLanguageModel the language model class.\n",
    "\n",
    "Rate limiting is a mechanism implemented by APIs to control the number of requests a user can make in a given time frame. This is done to prevent abuse so 1 player dont make the whole capacity used and slow.  However, rate limiting can be annoying because it can interrupt your workflow and force you to wait before making more requests.\n",
    "\n",
    "To make things easier, we've implemented a rate limit error controller in LargeLanguageModel class (see usage example below) that automatically adds sleep intervals between requests to avoid exceeding the rate limit. We will use the LargeLanguageModel class moving forward to make calls to Mistral AI. This class has the same logic as the examples we showed above but includes a mechanism to counter the rate limiting issue. \n",
    "\n",
    "Have a look into the class by ctrl+ clicking in the **LargeLanguageModel* name to jump directly to its implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from /Users/jean.machado@getyourguide.com/prj/rag-workshop/.env\n",
      "Hello! You can call me Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from chat_solution.llm import LargeLanguageModel\n",
    "\n",
    "# Initialize an instance of the LargeLanguageModel class\n",
    "llm = LargeLanguageModel()\n",
    "# Make a call to Mistral using the LargeLanguageModel class\n",
    "response = llm.call(\"hello! What is your name?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the LLM\n",
    "Now that we have seen how to make a basic call to the Mistral model using the [LargeLanguageModel](../chat_solution/llm.py) class, let's try some more prompts to see how the model responds to different types of queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Modifying Prompts\n",
    "Below are some example use cases of how to use an LLM such as Mistral. Play around with the prompts and see the results. Modify the prompts to see how the model's responses change. This will help you understand how to craft effective prompts and get the desired output from the model.\n",
    "\n",
    "Try to:\n",
    "- Ask different types of questions\n",
    "- Change the text for summarization or extraction (see examples 2 and 3 below)\n",
    "- Alter the style of the response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Asking for Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! 42Berlin is a coding school that follows the innovative educational model pioneered by 42 Network, which originated in France. Here are some key aspects of 42Berlin:\n",
      "\n",
      "### Educational Model\n",
      "1. **Peer-to-Peer Learning**: 42Berlin emphasizes a peer-to-peer learning environment where students learn from and teach each other. This model encourages collaboration and mutual support.\n",
      "\n",
      "2. **Project-Based Learning**: The curriculum is heavily focused on practical projects. Students work on real-world projects that help them develop both technical skills and problem-solving abilities.\n",
      "\n",
      "3. **No Teachers or Classes**: Unlike traditional schools, 42Berlin does not have teachers or formal classes. Instead, students learn through a combination of online resources, projects, and peer interactions.\n",
      "\n",
      "4. **24/7 Access**: The school is open 24 hours a day, 7 days a week, allowing students to work at their own pace and on their own schedule.\n",
      "\n",
      "### Admission Process\n",
      "1. **Piscine (Pool)**: The admission process begins with a four-week intensive bootcamp called \"Piscine.\" During this period, applicants are evaluated on their problem-solving skills, motivation, and ability to work in a team.\n",
      "\n",
      "2. **No Prerequisites**: 42Berlin does not require any formal education or prior coding experience. The Piscine is designed to assess potential rather than prior knowledge.\n",
      "\n",
      "### Curriculum\n",
      "1. **Core Programming Skills**: Students learn fundamental programming concepts and languages such as C, C++, and eventually move on to more advanced topics like web development, data structures, and algorithms.\n",
      "\n",
      "2. **Soft Skills**: In addition to technical skills, the program emphasizes the development of soft skills such as communication, teamwork, and project management.\n",
      "\n",
      "3. **Flexible Paths**: The curriculum is designed to be flexible, allowing students to explore different areas of interest within computer science.\n",
      "\n",
      "### Community and Support\n",
      "1. **Global Network**: As part of the 42 Network, 42Berlin students have access to a global community of peers, alumni, and industry professionals.\n",
      "\n",
      "2. **Mentorship**: While there are no traditional teachers, experienced students and alumni often mentor new students, providing guidance and support.\n",
      "\n",
      "### Outcomes\n",
      "1. **Job Placement**: 42Berlin has a strong focus on preparing students for careers in the tech industry. Many graduates go on to work for top tech companies.\n",
      "\n",
      "2. **Entrepreneurship**: The school also encourages entrepreneurship, and some graduates go on to start their own tech ventures.\n",
      "\n",
      "### Location\n",
      "42Berlin is located in Berlin, Germany, a city known for its vibrant tech scene and startup culture. This location provides students with access to numerous networking opportunities and potential job prospects.\n",
      "\n",
      "Overall, 42Berlin offers a unique and innovative approach to learning programming and computer science, focusing on practical skills, peer collaboration, and real-world projects.\n"
     ]
    }
   ],
   "source": [
    "# Example: Asking for information\n",
    "prompt = \"Can you tell me about coding school 42Berlin?\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Summarizing a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42Berlin is a tuition-free coding school empowering the next generation of coders through accessible and inclusive tech education.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_summarize = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Summarize the following text in one brief sentence: {text_to_summarize}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3: Extracting Information from a Given Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The year 42Berlin was founded is 2021.\n"
     ]
    }
   ],
   "source": [
    "# Change the text into something else to see the results\n",
    "text_to_extract_from = (\n",
    "    \"\"\"\n",
    "    42Berlin is a non-profit coding school offering software engineering education completely tuition free. \n",
    "    By making tech education more accessible and inclusive, they empower the next generation of coders.\n",
    "    Founded in 2021 and based in central Neukölln, we train our students up to the equivalent of Master’s level \n",
    "    and implement peer-learning methodologies that give autonomy to each student.\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "prompt = f\"Extract the year 42Berlin was founded from the following text: {text_to_extract_from}\"\n",
    "response = llm.call(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Task 3: Doing maths -> When will the LLM make a mistake?\n",
    "\n",
    "LLMs are trained on text language so they are not necessarily good with maths. We can be confident that it will fail to produce the correct result if you ask a question that is complex enough. On the notebook below we increase the complexity of the task at every run. Run it until you see it diverging.\n",
    "\n",
    "What was te number of iterations necessary to make it break?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "WARNING:root:Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}Waiting {time_to_wait} seconds before retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct result:  4\n",
      "Mistral result:  The result of 2 ** 2 is 4. In Python, the `**` operator is used for exponentiation, so 2 raised to the power of 2 equals 4.\n"
     ]
    }
   ],
   "source": [
    "if 'a' not in globals() or 'b' not in globals():\n",
    "    a = 1\n",
    "    b = 1\n",
    "a = a + 1\n",
    "b = b + 1\n",
    "\n",
    "## ** in python is the power operator meaning: a to the power of b\n",
    "print(\"Correct result: \", a**b)\n",
    "response = llm.call(f\"what is the results of {a} ** {b}\")\n",
    "print(\"Mistral result: \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you probably noted, mistral does not only return the results, it also explains it step by step. For example:\n",
    "\n",
    "```\n",
    "The result of 4 raised to the power of 4 (4 ** 4) is calculated as:\n",
    "4 * 4 * 4 * 4 = 256\n",
    "```\n",
    "\n",
    "Mistral and other language models deliberatelly add this longer explanations as they help the model commit less mistakes. This pattern is also known as chain of thought reasonsing. And is one of the most robust ways to improve LLM's responses. When you write prompts think about how to explain the steps of the reasonsing to improve the performance of your propmts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "LLMs sometimes generate responses that are plausible-sounding but factually incorrect or nonsensical. This phenomenon is known as \"hallucination\". \n",
    "Hallucination can occur because the model generates text based on patterns in the training data rather than actual knowledge or retrieval of relevant information.\n",
    "LLMs will produce the most likelly words that they would find in similar text, they have no clue about fact vs fiction. They can confidently produce writing about things they have no clue about.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Demonstrating Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will ask the model a question that it might to hallucinate an answer for, showing the limitations of relying solely on language generation without retrieval.\n",
    "\n",
    "Try running the command below a few times in a row and see how the response by the LLM changes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response likely to hallucinate:\n",
      "\n",
      "The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Beginners.\" This workshop aimed to introduce participants to the fundamentals of MLOps (Machine Learning Operations) and provide them with practical skills and knowledge to implement and manage machine learning projects effectively.\n"
     ]
    }
   ],
   "source": [
    "# Ask a question likely to cause hallucination\n",
    "prompt = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "response = llm.call(prompt)\n",
    "print(\"Response likely to hallucinate:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's move on to the next section on Retrieval-Augmented Generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that incorporates external information to a LLM to generate better responses. \n",
    "\n",
    "In RAG, a retrieval component searches and retrieves relevant information from a knowledge base or external documents, and a generation component uses this information to generate responses.\n",
    "This approach allows the model to access up-to-date information and provide more detailed and accurate answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Simple RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will demonstrate a simple example of how to use Retrieval-Augmented Generation. We will use a predefined set of documents, retrieve relevant information based on a query, and then generate a response using the retrieved information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(message: str, context: str):\n",
    "    \"\"\"\n",
    "    Message is the question that the user is asking.\n",
    "    Context is the information that we want to use to answer the question.\n",
    "    \"\"\"\n",
    "    return f\"\"\"Answer the question only using the provided content.\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        User Question: {message}\n",
    "\n",
    "        Be helpful and friendly. If the information cannot be found respond with \"I don't know\"\n",
    "        \"\"\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the code in the cell below, you can compare how our LLM responses differ by the information that you provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "ERROR:root:Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "ERROR:root:Waiting 2 seconds before retrying\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " The workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech was named \"MLOps for Women.\" This workshop aimed to empower women in the field of machine learning operations (MLOps) by providing them with practical skills, knowledge, and networking opportunities.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " I don't know.\n"
     ]
    }
   ],
   "source": [
    "# The workshop the MLOps Community hosted together with Girls in Tech Germany was called \"AI Launchpad: Building Your First Ml Pipeline\" or simply \"Building Your First ML Pipeline\"\n",
    "# We copy paste the info from our Eventbrite event page from the previous workshop and use this as context for the model to retrieve the right info from\n",
    "context = \"\"\"\n",
    "AI Launchpad - Building Your First ML Pipeline: \n",
    "On Wednesday, June 5th, 2024, the MLOps Community Berlin in collaboration with Girls in Tech Germany hosted an interactive workshop for beginners who want to kick start their career in AI/ML. \n",
    "The workshop starts at 18.00h at 42Berlin. \n",
    "\n",
    "🔍 Why Attend?\n",
    "\n",
    "Gain hands-on experience building your first ML pipeline in an agile way\n",
    "Apply the fundamentals of statistical modeling and basic Python\n",
    "Opportunities to improve your portfolio \n",
    "Connect with ML professionals at different levels of seniority\n",
    "\n",
    "\n",
    "✨ The Agenda: \n",
    "\n",
    "6:00 pm - Arrive & Pizza \n",
    "6:30 pm -  Introduction MLOps and GiT\n",
    "6:45 pm - Workshop Introduction\n",
    "7:30 pm - Break\n",
    "7:45 pm - Workshop\n",
    "9:45 pm - Networking\n",
    "\n",
    "\n",
    "🎉 Highlights:\n",
    "\n",
    "Food and drinks provided\n",
    "Engaging discussions and networking opportunities\n",
    "Bring your laptop and get ready to learn!\n",
    "\n",
    "\n",
    "💼 Who Should Attend?\n",
    "\n",
    "Individuals starting their career in Machine Learning or Artificial Intelligence\n",
    "Those looking to transition into the field of AI/ML\n",
    "Anyone interested in contributing to and learning from the ML community\n",
    "Don't miss out on this chance to gain practical AI/ML skills while expanding your professional network! \n",
    "\"\"\"\n",
    "\n",
    "message = \"What was the name of the workshop launched by the MLOps Community Berlin in collaboration with Girls in Tech?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error happened while calling the model: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Rate limit error: API error occurred: Status 429\n",
      "{\"message\":\"Requests rate limit exceeded\"}\n",
      "Waiting 2 seconds before retrying\n",
      "GENERIC RESPONSE:\n",
      " I'm an assistant that operates solely on the data it has been trained on up until 2021, and I don't have real-time web browsing capabilities or the ability to predict future weather with certainty. Therefore, I can't provide the specific weather for Berlin on the 10th of December, 2027.\n",
      "\n",
      "For the most accurate information, I recommend checking a reliable weather forecasting website or service closer to the date.\n",
      "------------------------------\n",
      "RAG RESPONSE:\n",
      " Hello! On the 10th of December 2027, the weather in Berlin is expected to be around 10 degrees Celsius.\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same thing with Berlin weather data!\n",
    "context = \"\"\"\n",
    "The weather in Berlin  December of 2027 will be around 13 degrees Celsius.\n",
    "Specific dates:\n",
    "- 10th of December: 10 degrees Celsius\n",
    "- 15th of December: 15 degrees Celsius\n",
    "- 20th of December: 7 degrees Celsius\n",
    "\"\"\"\n",
    "\n",
    "message = \"What will be the weather in Berlin on the 10th of December of 2027?\"\n",
    "\n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "Now try it yourself! Can you find some content on the internet (think, for example, news articles or very specific, locally relevant information that the LLM normally would not have access to). \n",
    "\n",
    "Play around with it and let the creative juices flow. Can you discover some more use cases for which you can use RAG can help make our LLM smarter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERIC RESPONSE:\n",
      " Hello! How can I assist you today?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_rag_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m generic_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcall(message)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGENERIC RESPONSE:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneric_response\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m rag_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_rag_prompt\u001b[49m(message\u001b[38;5;241m=\u001b[39mmessage, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m     13\u001b[0m rag_response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mcall(rag_prompt)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_rag_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"\"\"\n",
    "# Add your context here\n",
    "\"\"\" \n",
    "\n",
    "message = \"\"\"\n",
    "# Add your message here\n",
    "\"\"\" \n",
    "\n",
    "generic_response = llm.call(message)\n",
    "print(f\"GENERIC RESPONSE:\\n {generic_response}\")\n",
    "\n",
    "rag_prompt = create_rag_prompt(message=message, context=context)\n",
    "rag_response = llm.call(rag_prompt)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"RAG RESPONSE:\\n {rag_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's it! \n",
    "\n",
    "RAGs enrich the prompt with additional information about the topic to generate responses. The external information can come from various sources, such as PDFs, Google search results, social media posts, and more. With that, we’ve built a simple Q&A RAG.\n",
    "\n",
    "## ===> Now head to notebook 2 on embedddings and how llms undertand language\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1848951197487297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Steven Test Playground",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".virtualenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
